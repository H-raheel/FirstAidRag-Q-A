{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "!pip install pandas\n",
        "!pip install pdfplumber\n",
        "!pip install langchain\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence_transformers\n",
        "!pip install pypdf\n",
        "!pip install -U langchain-community\n",
        "!pip install -U langchain-huggingface\n",
        "!pip install transformers torch\n",
        "!pip install groq\n",
        "!pip install rank_bm25\n",
        "!pip install transformers accelerate bitsandbytes\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "1WGJgylDd0Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import fitz  # PyMuPDF\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from typing import Dict, List, Tuple\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from groq import Groq\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
        "import torch\n",
        "import traceback\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "8HS8RDNWDYHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Extraction\n",
        "def get_pdf_paths(folder_path):\n",
        "    \"\"\"\n",
        "    Returns a list of all PDF file paths in the given folder.\n",
        "    \"\"\"\n",
        "    return glob.glob(os.path.join(folder_path, \"*.pdf\"))\n",
        "\n",
        "def extract_data(pdf_paths):\n",
        "    all_data = []  # List to store extracted data for all PDFs\n",
        "\n",
        "    for pdf_path in pdf_paths:\n",
        "        pdf_data = {\"filename\": os.path.basename(pdf_path), \"text\": \"\", \"tables\": []}\n",
        "\n",
        "        # ----------- Extract Full Text with PyMuPDF ------------\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            for page_num in range(len(doc)):\n",
        "                page = doc.load_page(page_num)\n",
        "                pdf_data[\"text\"] += page.get_text()\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to read text from {pdf_path}: {e}\")\n",
        "\n",
        "        # ----------- Extract Tables with pdfplumber ------------\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    tables = page.extract_tables()\n",
        "                    pdf_data[\"tables\"].extend(tables)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to read tables from {pdf_path}: {e}\")\n",
        "\n",
        "        all_data.append(pdf_data)  # Add data for this PDF to the main list\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "M7ebYYBMDYDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from langchain.text_splitter import TokenTextSplitter, RecursiveCharacterTextSplitter\n",
        "\n",
        "# Ensure you have the necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function for token-based chunking\n",
        "def chunk_token_based(extracted_data, chunk_size=256, chunk_overlap=32):\n",
        "    \"\"\"\n",
        "    Token-based chunking using TokenTextSplitter.\n",
        "    \"\"\"\n",
        "    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    all_chunks = []\n",
        "    for data in extracted_data:\n",
        "        text_chunks = text_splitter.split_text(data['text'])\n",
        "        table_data = str(data.get('tables', []))\n",
        "        all_chunks.extend(text_chunks)\n",
        "        all_chunks.append(table_data)\n",
        "    return all_chunks\n",
        "\n",
        "# Function for paragraph-based chunking\n",
        "def chunk_paragraph_based(extracted_data, chunk_size=3, chunk_overlap=1):\n",
        "    \"\"\"\n",
        "    Paragraph-based chunking where chunks are created based on paragraphs.\n",
        "    \"\"\"\n",
        "    def paragraph_chunker(text):\n",
        "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "        chunks = []\n",
        "        for i in range(0, len(paragraphs), chunk_size - chunk_overlap):\n",
        "            chunk = \"\\n\\n\".join(paragraphs[i:i + chunk_size])\n",
        "            chunks.append(chunk)\n",
        "        return chunks\n",
        "\n",
        "    all_chunks = []\n",
        "    for data in extracted_data:\n",
        "        text = data.get(\"text\", \"\")\n",
        "        table_data = str(data.get(\"tables\", []))\n",
        "        text_chunks = paragraph_chunker(text)\n",
        "        all_chunks.extend(text_chunks)\n",
        "        all_chunks.append(table_data)\n",
        "    return all_chunks\n",
        "\n",
        "# Function for recursive character-based chunking\n",
        "def chunk_recursive_based(extracted_data, chunk_size=500, chunk_overlap=100):\n",
        "    \"\"\"\n",
        "    Recursive character-based chunking using RecursiveCharacterTextSplitter.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    all_chunks = []\n",
        "    for data in extracted_data:\n",
        "        text_chunks = text_splitter.split_text(data['text'])\n",
        "        table_data = str(data.get('tables', []))\n",
        "        all_chunks.extend(text_chunks)\n",
        "        all_chunks.append(table_data)\n",
        "    return all_chunks\n",
        "\n",
        "# Function for sentence-based chunking\n",
        "def chunk_sentence_based(extracted_data, chunk_size=5):\n",
        "    \"\"\"\n",
        "    Sentence-based chunking, where each chunk contains chunk_size sentences.\n",
        "    \"\"\"\n",
        "    def sentence_chunker(text):\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        chunks = []\n",
        "        for i in range(0, len(sentences), chunk_size):\n",
        "            chunk = \" \".join(sentences[i:i + chunk_size])\n",
        "            chunks.append(chunk)\n",
        "        return chunks\n",
        "\n",
        "    all_chunks = []\n",
        "    for data in extracted_data:\n",
        "        text = data.get(\"text\", \"\")\n",
        "        table_data = str(data.get(\"tables\", []))\n",
        "        text_chunks = sentence_chunker(text)\n",
        "        all_chunks.extend(text_chunks)\n",
        "        all_chunks.append(table_data)\n",
        "    return all_chunks\n",
        "\n",
        "# Main function to choose the chunking method based on user input\n",
        "def chunk_texts(extracted_data, chunk_type='recursive', chunk_size=500, chunk_overlap=100):\n",
        "    \"\"\"\n",
        "    Main function that selects the chunking method based on user input.\n",
        "    \"\"\"\n",
        "    if chunk_type == 'token':\n",
        "        return chunk_token_based(extracted_data, chunk_size, chunk_overlap)\n",
        "    elif chunk_type == 'paragraph':\n",
        "        return chunk_paragraph_based(extracted_data, chunk_size, chunk_overlap)\n",
        "    elif chunk_type == 'recursive':\n",
        "        return chunk_recursive_based(extracted_data, chunk_size, chunk_overlap)\n",
        "    elif chunk_type == 'sentence':\n",
        "        return chunk_sentence_based(extracted_data, chunk_size)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid chunk_type. Choose from 'token', 'paragraph', 'recursive', or 'sentence'.\")"
      ],
      "metadata": {
        "id": "87qCqKr8DYBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the embedding model\n",
        "def init_embedding_model():\n",
        "    return HuggingFaceEmbeddings(\n",
        "        model_name=\"pritamdeka/S-PubMedBERT-MS-MARCO\",\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "\n",
        "# Embed all chunks\n",
        "def embed_all_chunks(embedding_model, chunks):\n",
        "    return embedding_model.embed_documents(chunks)\n",
        "\n",
        "# Build FAISS index\n",
        "def build_faiss_index(embeddings):\n",
        "    dimension = len(embeddings[0])\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(np.array(embeddings).astype(\"float32\"))\n",
        "    return index"
      ],
      "metadata": {
        "id": "FCLoDLcQDX-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query functions\n",
        "def semantic_search(query, index, embedding_model, all_chunks, top_k=3):\n",
        "    \"\"\"Pure semantic search using FAISS\"\"\"\n",
        "    query_embedding = np.array([embedding_model.embed_query(query)]).astype('float32')\n",
        "    _, indices = index.search(query_embedding, top_k)\n",
        "    return [all_chunks[i] for i in indices[0]]\n",
        "\n",
        "def bm25_search(query, tokenized_corpus, bm25, all_chunks, top_k=3):\n",
        "    \"\"\"Pure BM25 search using pre-initialized index\"\"\"\n",
        "    tokenized_query = word_tokenize(query.lower())\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    indices = np.argsort(scores)[-top_k:][::-1]\n",
        "    return [all_chunks[i] for i in indices]\n",
        "\n",
        "def mmr_search(query, embedding_model, index, all_chunks, top_k=3, diversity=0.7):\n",
        "    \"\"\"Maximal Marginal Relevance search\"\"\"\n",
        "    query_embedding = np.array([embedding_model.embed_query(query)]).astype('float32')\n",
        "    _, indices = index.search(query_embedding, top_k*2)  # Get extra candidates\n",
        "\n",
        "    # Implement MMR diversification\n",
        "    selected = []\n",
        "    candidates = [all_chunks[i] for i in indices[0]]\n",
        "    candidate_embeddings = [embedding_model.embed_query(doc) for doc in candidates]\n",
        "    query_embedding = query_embedding[0]  # Unwrap from array\n",
        "\n",
        "    while len(selected) < top_k and candidates:\n",
        "        scores = []\n",
        "        for i, (doc, doc_embedding) in enumerate(zip(candidates, candidate_embeddings)):\n",
        "            # Calculate similarity to query\n",
        "            sim_score = np.dot(query_embedding, doc_embedding)\n",
        "            if selected:\n",
        "                # Calculate max redundancy with already selected docs\n",
        "                selected_embeddings = [embedding_model.embed_query(s) for s in selected]\n",
        "                max_redun = max(np.dot(doc_embedding, sel_emb) for sel_emb in selected_embeddings)\n",
        "                scores.append(diversity * sim_score - (1 - diversity) * max_redun)\n",
        "            else:\n",
        "                scores.append(sim_score)\n",
        "\n",
        "        best_idx = np.argmax(scores)\n",
        "        selected.append(candidates.pop(best_idx))\n",
        "        candidate_embeddings.pop(best_idx)\n",
        "\n",
        "    return selected\n",
        "\n",
        "# Hybrid methods\n",
        "def hybrid_semantic_bm25(query, index, embedding_model, bm25, tokenized_corpus, all_chunks, top_k=3):\n",
        "    \"\"\"Combine semantic and BM25 results\"\"\"\n",
        "    semantic = semantic_search(query, index, embedding_model, all_chunks, top_k)\n",
        "    bm25_results = bm25_search(query, tokenized_corpus, bm25, all_chunks, top_k)\n",
        "    return list(dict.fromkeys(semantic + bm25_results))[:top_k*2]\n",
        "\n",
        "def hybrid_mmr_semantic(query, index, embedding_model, all_chunks, top_k=3):\n",
        "    \"\"\"Combine MMR and semantic results\"\"\"\n",
        "    mmr = mmr_search(query, embedding_model, index, all_chunks, top_k)\n",
        "    semantic = semantic_search(query, index, embedding_model, all_chunks, top_k)\n",
        "    return list(dict.fromkeys(mmr + semantic))[:top_k*2]\n"
      ],
      "metadata": {
        "id": "5IqM6hCmDX8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== GROQ SUMMARIZATION ==================\n",
        "SUMMARIZATION_PROMPT = \"\"\"\n",
        "Please summarize the following medical text while preserving all critical information.\n",
        "Keep medical terminology accurate and maintain all important steps or recommendations.\n",
        "Focus on preserving:\n",
        "- Step-by-step procedures\n",
        "- Dosage information\n",
        "- Warning signs\n",
        "- Key recommendations\n",
        "\n",
        "Text to summarize:\n",
        "{text}\n",
        "\n",
        "Concise summary (250-300 words):\n",
        "\"\"\"\n",
        "\n",
        "def summarize_text(text, client, model):\n",
        "    \"\"\"Summarize text using Groq API\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": SUMMARIZATION_PROMPT.format(text=text)\n",
        "            }],\n",
        "            model=model,\n",
        "            temperature=0.3,\n",
        "            max_tokens=400\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Summarization failed: {str(e)}\")\n",
        "        return text[:500]  # Fallback to truncation if summarization fails\n",
        "\n",
        "def summarize_context(context, client, max_length=300):\n",
        "    \"\"\"Summarize retrieved context using Groq\"\"\"\n",
        "    model='allam-2-7b'\n",
        "    combined = \"\\n\".join(context)\n",
        "\n",
        "    # First try to summarize with Groq\n",
        "    summary = summarize_text(combined, client, model)\n",
        "\n",
        "    # Fallback to simple truncation if summary is too long\n",
        "    if len(summary) > max_length * 1.5:  # Allow some overflow\n",
        "        summary = \". \".join([s.strip() for s in combined.split(\".\")[:5]]) + \".\"\n",
        "        summary = summary[:max_length]\n",
        "\n",
        "    return [summary]"
      ],
      "metadata": {
        "id": "9fKCchLmDX5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\"\"\"\n",
        "\n",
        "JUDGE_PROMPT_TEMPLATE = \"\"\"\n",
        "You are a judge evaluating a question-answering system.\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "<answer_generated>\n",
        "{answer}\n",
        "</answer_generated>\n",
        "\n",
        "<golden_reference>\n",
        "{golden}\n",
        "</golden_reference>\n",
        "\n",
        "Evaluate the generated answer on a scale of 1 to 5 (5 being highest) for the following:\n",
        "\n",
        "Faithfulness: Does the generated answer stay factually consistent with the golden reference?\n",
        "Relevance: Does the generated answer actually answer the question?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jxN42HktDX3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== UPDATED CORE FUNCTIONS ==================\n",
        "def ask_groq(query, context, client, model):\n",
        "    \"\"\"Model-aware generation\"\"\"\n",
        "    prompt = PROMPT_TEMPLATE.format(\n",
        "        context=\"\\n\".join(context),\n",
        "        question=query\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        model=model,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def evaluate_with_groq_judge(question, answer, golden, client, model):\n",
        "    \"\"\"Model-aware evaluation\"\"\"\n",
        "    prompt = JUDGE_PROMPT_TEMPLATE.format(\n",
        "        question=question,\n",
        "        answer=answer,\n",
        "        golden=golden\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        model=model,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "hOEnUuqQDX04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LocalModelWrapper:\n",
        "    def __init__(self, model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        self.model_name = model_name\n",
        "        self.device = device\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the model and tokenizer with 4-bit quantization\"\"\"\n",
        "        print(f\"Loading {self.model_name}...\")\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            padding_side=\"left\",  # Important for generation\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Set pad token if not defined\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Configure quantization for memory efficiency\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True  # Additional memory savings\n",
        "        )\n",
        "\n",
        "        # Load model with quantization\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            quantization_config=quant_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        print(f\"{self.model_name} loaded successfully!\")\n",
        "\n",
        "    def generate(self, prompt, max_length=500, temperature=0.7, max_input_length=1024):\n",
        "        \"\"\"Generate text from prompt with model-specific formatting\"\"\"\n",
        "        # Apply model-specific prompt formatting\n",
        "        if \"falcon\" in self.model_name.lower():\n",
        "            prompt = f\"User: {prompt}\\nAssistant:\"\n",
        "        elif \"qwen\" in self.model_name.lower():\n",
        "            prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\"\n",
        "        else:  # Default format\n",
        "            prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "\n",
        "        # Tokenize with truncation\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=max_input_length\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Configure generation\n",
        "        generation_config = GenerationConfig(\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "            eos_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Generate text\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                generation_config=generation_config\n",
        "            )\n",
        "\n",
        "        # Decode and clean output\n",
        "        full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Remove input prompt from output\n",
        "        generated_text = full_output[len(prompt):].strip()\n",
        "        return generated_text"
      ],
      "metadata": {
        "id": "IAxZkDysF073"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGEvaluator:\n",
        "    def __init__(self,\n",
        "                 embedding_model,\n",
        "                 all_chunks,\n",
        "                 index,\n",
        "                 groq_client=None,\n",
        "                 index_model=\"S-PubMedBERT-MS-MARCO\",\n",
        "                 gen_model=\"llama3-8b-8192\",\n",
        "                 eval_model=\"deepseek-r1-distill-llama-70b\",\n",
        "                 summary_model=\"allam-2-7b\"):\n",
        "\n",
        "        self.embedding_model = embedding_model\n",
        "        self.all_chunks = all_chunks\n",
        "        self.index = index\n",
        "        self.client = groq_client\n",
        "        self.timing_data = []\n",
        "        self.cache = {}\n",
        "        self.models = {\n",
        "            \"index\": index_model,\n",
        "            \"gen\": gen_model,\n",
        "            \"eval\": eval_model,\n",
        "            \"summary\": summary_model\n",
        "        }\n",
        "\n",
        "        # Initialize local models if specified\n",
        "        self.local_model = None\n",
        "        if any(m in gen_model for m in [\"tiiuae/Falcon3-3B-Base\", \"Qwen/Qwen2.5-3B\"]):\n",
        "            self.local_model = LocalModelWrapper(gen_model)\n",
        "\n",
        "        # Initialize retrieval components\n",
        "        self._init_retrieval()\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Clean up models when evaluator is destroyed\"\"\"\n",
        "        if self.local_model:\n",
        "            del self.local_model\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def _init_retrieval(self):\n",
        "        \"\"\"Initialize retrieval systems once\"\"\"\n",
        "        try:\n",
        "            # Tokenize corpus for BM25\n",
        "            self.tokenized_corpus = [word_tokenize(chunk.lower()) for chunk in self.all_chunks]\n",
        "            self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing retrieval components: {str(e)}\")\n",
        "            self.tokenized_corpus = []\n",
        "            self.bm25 = None\n",
        "\n",
        "    def get_filename(self):\n",
        "        \"\"\"Generate filename with timestamp and model info\"\"\"\n",
        "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        return (f\"rag_results_{timestamp}_\"\n",
        "                f\"index_{self.models['index']}_\"\n",
        "                # f\"gen_{self.models['gen'].replace('/','-')}_\"\n",
        "                f\"eval_{self.models['eval']}.csv\")\n",
        "\n",
        "    def ask_model(self, query, context):\n",
        "        \"\"\"Unified method to query either Groq or local models\"\"\"\n",
        "        prompt = PROMPT_TEMPLATE.format(\n",
        "            context=\"\\n\".join(context),\n",
        "            question=query\n",
        "        )\n",
        "\n",
        "        # Use local model if available\n",
        "        if self.local_model:\n",
        "            try:\n",
        "                start_gen = time.time()\n",
        "                answer = self.local_model.generate(prompt)\n",
        "                gen_time = time.time() - start_gen\n",
        "                return answer, gen_time\n",
        "            except torch.cuda.OutOfMemoryError:\n",
        "                return \"ERROR: GPU Out of Memory\", 0\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e).lower():\n",
        "                    return \"ERROR: GPU Out of Memory\", 0\n",
        "                return f\"ERROR: {str(e)}\", 0\n",
        "\n",
        "        # Use Groq if no local model\n",
        "        elif self.client:\n",
        "            try:\n",
        "                start_gen = time.time()\n",
        "                response = self.client.chat.completions.create(\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    model=self.models['gen'],\n",
        "                )\n",
        "                gen_time = time.time() - start_gen\n",
        "                return response.choices[0].message.content, gen_time\n",
        "            except Exception as e:\n",
        "                return f\"ERROR: {str(e)}\", 0\n",
        "\n",
        "        return \"ERROR: No generation backend available\", 0\n",
        "\n",
        "    def timed_retrieve(self, query, strategy_name, k):\n",
        "        \"\"\"Cached retrieval with timing\"\"\"\n",
        "        cache_key = f\"{strategy_name}_{query}_{k}\"\n",
        "\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key], 0.0, None\n",
        "\n",
        "        start = time.time()\n",
        "        result = []\n",
        "        error_message = None\n",
        "\n",
        "        try:\n",
        "            if strategy_name == \"semantic\":\n",
        "                result = semantic_search(query, self.index, self.embedding_model, self.all_chunks, k)\n",
        "            elif strategy_name == \"bm25\":\n",
        "                result = bm25_search(query, self.tokenized_corpus, self.bm25, self.all_chunks, k)\n",
        "            elif strategy_name == \"mmr\":\n",
        "                result = mmr_search(query, self.embedding_model, self.index, self.all_chunks, k)\n",
        "            elif strategy_name == \"hybrid_semantic_bm25\":\n",
        "                result = hybrid_semantic_bm25(query, self.index, self.embedding_model, self.bm25,\n",
        "                                            self.tokenized_corpus, self.all_chunks, k)\n",
        "            elif strategy_name == \"hybrid_mmr_semantic\":\n",
        "                result = hybrid_mmr_semantic(query, self.index, self.embedding_model, self.all_chunks, k)\n",
        "            elif strategy_name.endswith(\"_summarized\"):\n",
        "                base_strategy = strategy_name.replace(\"_summarized\", \"\")\n",
        "                base_results, _, _ = self.timed_retrieve(query, base_strategy, k)\n",
        "                result = summarize_context(base_results, self.client)\n",
        "            else:\n",
        "                error_message = f\"Unknown strategy: {strategy_name}\"\n",
        "        except Exception as e:\n",
        "            error_message = f\"Retrieval error ({strategy_name}): {str(e)}\"\n",
        "\n",
        "        retrieval_time = time.time() - start\n",
        "        self.cache[cache_key] = result\n",
        "        return result, retrieval_time, error_message\n",
        "\n",
        "    def evaluate_strategy(self, query, golden, strategy_name, k):\n",
        "        \"\"\"Complete evaluation pipeline for one strategy\"\"\"\n",
        "        result_data = {\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"query\": query,\n",
        "            \"golden_answer\": golden,\n",
        "            \"strategy\": strategy_name,\n",
        "            \"k\": k,\n",
        "            \"index_model\": self.models['index'],\n",
        "            \"gen_model\": self.models['gen'],\n",
        "            \"eval_model\": self.models['eval'],\n",
        "            \"summary_model\": self.models['summary'],\n",
        "            \"context\": \"\",\n",
        "            \"context_length\": 0,\n",
        "            \"retrieval_time\": 0,\n",
        "            \"generated_answer\": \"\",\n",
        "            \"gen_time\": 0,\n",
        "            \"evaluation_response\": \"\",\n",
        "            \"eval_time\": 0,\n",
        "            \"faithfulness\": \"N/A\",\n",
        "            \"relevance\": \"N/A\",\n",
        "            \"total_time\": 0,\n",
        "            \"errors\": \"\"\n",
        "        }\n",
        "\n",
        "        errors = []\n",
        "\n",
        "        # 1. Retrieval\n",
        "        try:\n",
        "            context, ret_time, error = self.timed_retrieve(query, strategy_name, k)\n",
        "            result_data[\"retrieval_time\"] = ret_time\n",
        "            result_data[\"context_length\"] = sum(len(c) for c in context) if context else 0\n",
        "            result_data[\"context\"] = \" ||| \".join(context) if context else \"\"\n",
        "\n",
        "            if error:\n",
        "                errors.append(error)\n",
        "                context = []  # Ensure empty context if retrieval failed\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Retrieval exception: {str(e)}\")\n",
        "            context = []\n",
        "\n",
        "        # 2. Generation\n",
        "        try:\n",
        "            answer, gen_time = self.ask_model(query, context)\n",
        "            result_data[\"generated_answer\"] = answer\n",
        "            result_data[\"gen_time\"] = gen_time\n",
        "            if \"ERROR\" in answer:\n",
        "                errors.append(answer)\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Generation error: {str(e)}\")\n",
        "            result_data[\"generated_answer\"] = f\"ERROR: {str(e)}\"\n",
        "            result_data[\"gen_time\"] = 0\n",
        "\n",
        "        # 3. Evaluation (skip if generation failed)\n",
        "        if not any(\"ERROR\" in result_data[\"generated_answer\"] for e in errors):\n",
        "            try:\n",
        "                start_eval = time.time()\n",
        "                eval_text = evaluate_with_groq_judge(\n",
        "                    query,\n",
        "                    result_data[\"generated_answer\"],\n",
        "                    golden,\n",
        "                    self.client,\n",
        "                    model=self.models['eval']\n",
        "                )\n",
        "                eval_time = time.time() - start_eval\n",
        "                result_data[\"evaluation_response\"] = eval_text\n",
        "                result_data[\"eval_time\"] = eval_time\n",
        "\n",
        "                # Parse scores\n",
        "                try:\n",
        "                    result_data[\"faithfulness\"] = eval_text.split(\"Faithfulness:\")[1].split()[0].strip()\n",
        "                    result_data[\"relevance\"] = eval_text.split(\"Relevance:\")[1].split()[0].strip()\n",
        "                except:\n",
        "                    errors.append(\"Failed to parse evaluation scores\")\n",
        "            except Exception as e:\n",
        "                errors.append(f\"Evaluation error: {str(e)}\")\n",
        "\n",
        "        # Calculate total time\n",
        "        result_data[\"total_time\"] = (\n",
        "            result_data[\"retrieval_time\"] +\n",
        "            result_data[\"gen_time\"] +\n",
        "            result_data.get(\"eval_time\", 0)\n",
        "        )\n",
        "\n",
        "        # Log errors\n",
        "        if errors:\n",
        "            result_data[\"errors\"] = \" | \".join(errors)\n",
        "\n",
        "        return result_data\n",
        "\n",
        "    def run_evaluations(self, queries, k_values, strategies=None):\n",
        "        \"\"\"Evaluate all strategies across k values for multiple queries\"\"\"\n",
        "        if strategies is None:\n",
        "            strategies = [\n",
        "                \"semantic\",\n",
        "                \"bm25\",\n",
        "                \"mmr\",\n",
        "                \"hybrid_semantic_bm25\",\n",
        "                \"hybrid_mmr_semantic\",\n",
        "                \"semantic_summarized\",\n",
        "                \"bm25_summarized\",\n",
        "                \"mmr_summarized\",\n",
        "                \"hybrid_semantic_bm25_summarized\",\n",
        "                \"hybrid_mmr_semantic_summarized\"\n",
        "            ]\n",
        "\n",
        "        for query, golden in queries.items():\n",
        "            print(f\"\\nEvaluating query: {query[:60]}...\")\n",
        "            for k in k_values:\n",
        "                for strategy in strategies:\n",
        "                    try:\n",
        "                        result = self.evaluate_strategy(query, golden, strategy, k)\n",
        "                        self.timing_data.append(result)\n",
        "\n",
        "                        # Print status\n",
        "                        status = (\n",
        "                            f\"{strategy}(k={k}): {result['total_time']:.1f}s, \"\n",
        "                            f\"Faith={result['faithfulness']}, Rel={result['relevance']}\"\n",
        "                        )\n",
        "                        if result['errors']:\n",
        "                            status += f\" [ERRORS: {result['errors'][:50]}...]\"\n",
        "                        print(status)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        error_data = {\n",
        "                            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                            \"query\": query,\n",
        "                            \"golden_answer\": golden,\n",
        "                            \"strategy\": strategy,\n",
        "                            \"k\": k,\n",
        "                            \"errors\": f\"Critical error: {str(e)}\",\n",
        "                            \"faithfulness\": \"ERROR\",\n",
        "                            \"relevance\": \"ERROR\"\n",
        "                        }\n",
        "                        self.timing_data.append(error_data)\n",
        "                        print(f\"{strategy}(k={k}): CRITICAL ERROR - {str(e)}\")\n",
        "\n",
        "    def save_results(self, filename=None):\n",
        "        \"\"\"Save comprehensive results with error handling\"\"\"\n",
        "        if not self.timing_data:\n",
        "            print(\"Warning: No results to save\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            df = pd.DataFrame(self.timing_data)\n",
        "\n",
        "            # Ensure consistent column order\n",
        "            columns = [\n",
        "                'timestamp', 'query', 'golden_answer', 'strategy', 'k',\n",
        "                'index_model', 'gen_model', 'eval_model', 'summary_model',\n",
        "                'context_length', 'retrieval_time', 'gen_time', 'eval_time',\n",
        "                'total_time', 'faithfulness', 'relevance', 'errors',\n",
        "                'context', 'generated_answer', 'evaluation_response'\n",
        "            ]\n",
        "            df = df.reindex(columns=[c for c in columns if c in df.columns])\n",
        "\n",
        "            filename = filename or self.get_filename()\n",
        "            df.to_csv(filename, index=False)\n",
        "            print(f\"Saved {len(df)} results to {filename}\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save results: {str(e)}\")\n",
        "            try:\n",
        "                # Emergency save\n",
        "                backup_file = f\"rag_results_backup_{int(time.time())}.csv\"\n",
        "                pd.DataFrame(self.timing_data).to_csv(backup_file, index=False)\n",
        "                print(f\"Saved backup to {backup_file}\")\n",
        "            except:\n",
        "                print(\"Critical: Could not save backup!\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "OUCiateBDXyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FIRSTAID_QA = {\n",
        "    \"What specific adjustments should be made when performing CPR on a child compared to an adult, in terms of hand placement, compression depth, head positioning, and the use of rescue breaths?\":\n",
        "    \"\"\"\n",
        "    While the overall CPR process is similar for both adults and children—consisting of chest compressions and rescue breaths—there are a few important distinctions that take the size and physical development of the person into account. For adults, CPR is performed with two hands placed one over the other on the center of the chest, compressing at least 2 inches deep at a rate of 100–120 compressions per minute. The head is tilted to a past-neutral position to open the airway for rescue breaths, and each breath should last about one second, making the chest rise visibly.\n",
        "\n",
        "    For children, the approach is slightly gentler. The compression depth is about 2 inches rather than at least 2 inches, and in the case of a smaller child, only one hand may be needed for chest compressions instead of two. The airway is opened by tilting the head to a slightly past-neutral position, which is less extreme than the tilt for adults. However, the rate of compressions remains the same—100 to 120 per minute—and rescue breaths are also given after every 30 compressions, just like with adults.\n",
        "\n",
        "    These adjustments help avoid injury while still ensuring that CPR is effective for a smaller and more delicate body.\n",
        "    \"\"\",\n",
        "\n",
        "    \"What are the steps for providing first aid to a choking child or baby, and what actions should you take if the child or baby becomes unresponsive?\":\n",
        "    \"\"\"\n",
        "    If a child is choking and able to cough or speak, encourage them to continue coughing as this means their airway is not completely blocked. However, if the airway becomes fully obstructed, first aid must be provided immediately. Start by delivering back blows, placing your arm across the child’s chest, bending them forward, and giving up to five firm blows between the shoulder blades. If the object is not dislodged, move on to abdominal thrusts, where you place your fist just above the belly button and give five quick inward and upward thrusts. If the blockage persists, use chest thrusts, positioning your fist in the center of the chest and giving five thrusts by pulling straight back. If the child becomes unresponsive, ensure EMS is called and start CPR, beginning with chest compressions.\n",
        "    \"\"\",\n",
        "\n",
        "    \"In what circumstances should each type of sling—arm sling, elevated sling, and collar-and-cuff—be used, and what precautions must be taken to ensure proper limb support and circulation in both standard and improvised first aid situations?\":\n",
        "    \"\"\"\n",
        "    Slings are used to support an injured arm or to assist in the treatment of injuries such as fractured ribs, but should only be applied if they do not cause the casualty pain. There are three main types of slings: the arm sling, elevated sling, and collar-and-cuff (clove hitch), each used based on the location of the injury. An arm sling is suitable for forearm injuries and should hold the forearm parallel to the ground, with the wrist slightly higher than the elbow. The elevated sling is used for shoulder injuries and supports the hand across the chest towards the uninjured side. The collar-and-cuff is ideal for upper arm injuries or as added support for fractured ribs, allowing the elbow to hang naturally while the wrist is secured with a clove hitch.\n",
        "\n",
        "    While triangular bandages are ideal, in emergency situations, other materials like ties, belts, or even shirts can be improvised. After applying any sling, it’s crucial to check for proper circulation by feeling for a pulse at the wrist or using the fingernail color test. Slings must always be positioned comfortably, and the injured limb should never be forced into any sling. Additional support, if needed, can be provided by securing the arm against the chest using more triangular bandages.\n",
        "    \"\"\",\n",
        "\n",
        "    \"What are the causes, symptoms, and appropriate first aid treatment for hypothermia caused by cold exposure?\":\n",
        "    \"\"\"\n",
        "    Hypothermia is a serious cold-related emergency that occurs when the body temperature drops below 35°C due to prolonged exposure to cold environments. It commonly affects elderly individuals, young children, and those with impaired mental or physical abilities, especially when they are unable to access warmth or seek help. Risk situations include being caught in bad weather, wearing wet clothing, or lacking proper heating. Early signs of hypothermia include pale, cold skin, uncontrollable shivering, fatigue, poor coordination, and confusion. As the condition worsens, symptoms may progress to slow breathing, blurred vision, muscle stiffness, and unconsciousness, with the person possibly appearing asleep or even dead. Immediate care involves calling emergency services, moving the person to a warmer place, removing wet clothing, and wrapping them in blankets or a space blanket. If conscious, warm sweet drinks can be given. However, the person must be warmed slowly—avoiding direct heat, hot baths, rubbing the skin, or giving alcohol—as these can worsen the condition. Prompt action is crucial to prevent serious complications or death.\n",
        "    \"\"\",\n",
        "\n",
        "    \"What lifestyle habits can help manage high blood pressure and reduce related health risks?\":\n",
        "    \"\"\"\n",
        "    The treatment and prevention of high blood pressure largely depend on its severity and any other existing medical conditions. Doctors often recommend lifestyle modifications as a key part of managing and preventing high blood pressure. Maintaining a healthy body weight is crucial, as excess weight can put additional strain on the heart. A well-balanced diet rich in fresh fruits, vegetables, and low-fat dairy products is also essential, while high-fat and high-cholesterol foods should be avoided to reduce the risk of atherosclerosis. It is important to limit salt intake, as sodium causes fluid retention and increases the heart’s workload. Adequate consumption of nutrients like potassium, magnesium, and calcium is recommended, with fruits—particularly citrus—being good sources of potassium. Regular physical activity is another important preventive measure; aerobic exercises such as walking or swimming should be done for at least 30 to 45 minutes, five times a week. Additionally, quitting smoking is strongly advised, as smoking constricts blood vessels, raises blood pressure, and is one of the most serious risk factors for cardiovascular diseases. Together, these measures play a significant role in effectively managing high blood pressure and promoting heart health.\n",
        "    \"\"\",\n",
        "\n",
        "    \"What are the steps to revive a person if they have had a heart attack ?\":\n",
        "    \"\"\"\n",
        "    To revive a heart attack victim, you must act quickly and follow these key steps:\n",
        "\n",
        "    Lay the person flat on their back on a firm surface, ensuring their neck is supported and their face is facing upward.\n",
        "\n",
        "    Check responsiveness by gently tapping their shoulders. If unresponsive, immediately call emergency services (e.g., 999) or ask someone nearby to do so.\n",
        "\n",
        "    Open their airway by tilting the head back and lifting the chin. If there’s something blocking the mouth or throat, remove it carefully.\n",
        "\n",
        "    Check for breathing by placing your ear near their mouth and watching their chest for movement—for no more than 10 seconds.\n",
        "\n",
        "    If the person is not breathing, begin mouth-to-mouth resuscitation:\n",
        "\n",
        "    Pinch the nose shut, give 2 deep breaths into their mouth (for adults/children) or 2 short puffs (for infants), watching the chest rise.\n",
        "\n",
        "    Check for a pulse by feeling the side of the neck with two fingers for up to 10 seconds.\n",
        "\n",
        "    If there is no pulse, begin chest compressions:\n",
        "\n",
        "    Place one hand over the other in the center of the chest, keep elbows straight, and press down firmly about 4–5 cm deep.\n",
        "\n",
        "    Perform 30 compressions followed by 2 breaths, continuing at a steady pace. Aim for 9 cycles in 2 minutes, checking for a pulse every few cycles.\n",
        "\n",
        "    As soon as a pulse returns, stop compressions and check if the person is breathing. If not, continue rescue breaths until normal breathing resumes.\n",
        "    \"\"\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "54MZEZptDXv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get GROQ API key from user data (or set directly)\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)"
      ],
      "metadata": {
        "id": "x5U8vknLEAOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the data pipeline\n",
        "folder_path = \"/content/data\"  # Update this to your folder\n",
        "pdf_paths = get_pdf_paths(folder_path)\n",
        "extracted_data = extract_data(pdf_paths)\n"
      ],
      "metadata": {
        "id": "MX3xubmxEBu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_chunks = chunk_texts(extracted_data, chunk_type='recursive', chunk_size=512, chunk_overlap=100)"
      ],
      "metadata": {
        "id": "-gXIq_DtbeM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize embedding model, embed all chunks, and build index\n",
        "embedding_model = init_embedding_model()\n",
        "embeddings = embed_all_chunks(embedding_model, all_chunks)\n",
        "faiss_index = build_faiss_index(embeddings)\n",
        "\n",
        "# Save index and chunks for future use if needed\n",
        "faiss.write_index(faiss_index, \"faiss_index.idx\")"
      ],
      "metadata": {
        "id": "x0fSaL8oEDhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of k values to test\n",
        "k_values = [3, 5, 7]\n",
        "\n",
        "# List of generation models to test\n",
        "gen_models = [\n",
        "    \"llama3-70b-8192\",\n",
        "    \"tiiuae/Falcon3-3B-Base\",\n",
        "    \"Qwen/Qwen2.5-3B\"\n",
        "]\n",
        "\n",
        "# Evaluation model to use (consistent across runs)\n",
        "eval_model = \"deepseek-r1-distill-llama-70b\"\n",
        "\n",
        "# Initialize a DataFrame to store all results\n",
        "all_results = []"
      ],
      "metadata": {
        "id": "GsYrwQlMEF3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through all generation models\n",
        "for gen_model in gen_models:\n",
        "    print(f\"\\n===== Testing Generation Model: {gen_model} =====\\n\")\n",
        "\n",
        "    try:\n",
        "        # Set up evaluator with the current generation model\n",
        "        evaluator = RAGEvaluator(\n",
        "            embedding_model=embedding_model,\n",
        "            all_chunks=all_chunks,\n",
        "            index=faiss_index,\n",
        "            groq_client=groq_client,\n",
        "            index_model=\"S-PubMedBERT-MS-MARCO\",\n",
        "            gen_model=gen_model,\n",
        "            eval_model=eval_model\n",
        "        )\n",
        "\n",
        "        # Run evaluations with all k values\n",
        "        evaluator.run_evaluations(\n",
        "            queries=FIRSTAID_QA,\n",
        "            k_values=k_values\n",
        "        )\n",
        "\n",
        "        # Save individual results for this model\n",
        "        results_df = evaluator.save_results()\n",
        "\n",
        "        # Only append if the DataFrame is not empty\n",
        "        if results_df is not None and not results_df.empty:\n",
        "            all_results.append(results_df)\n",
        "        else:\n",
        "            print(f\"⚠️ No results returned for model {gen_model}, skipping.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error encountered while processing model {gen_model}:\\n{e}\")\n",
        "        traceback.print_exc()\n",
        "        continue  # Continue to the next model\n",
        "\n",
        "# Combine all results into one DataFrame\n",
        "if all_results:\n",
        "    combined_results = pd.concat(all_results, ignore_index=True)\n",
        "    combined_results.to_csv(\"combined_rag_comparison.csv\", index=False)\n",
        "    print(\"All results saved to combined_rag_comparison.csv\")\n",
        "else:\n",
        "    print(\"No valid results to save. CSV not created.\")\n"
      ],
      "metadata": {
        "id": "FEf7SiGYDXt2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}